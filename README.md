# Globe VP Data - Dimension Magasin Unique et HistorisÃ©e

**Projet:** Consolidation multi-sources avec DataOps complet

**Stack:** dbt, Snowflake, Flyway, GitHub Actions

**Auteur:** Abdelfattah Abouelaoualim

**DerniÃ¨re mise Ã  jour:** 2025-11-09

---

## Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture Technique](#architecture-technique)
3. [Politique de Gouvernance et SÃ©curitÃ© (RBAC)](#politique-de-gouvernance-et-sÃ©curitÃ©-rbac)
4. [Gestion de la QualitÃ© et ObservabilitÃ©](#gestion-de-la-qualitÃ©-et-observabilitÃ©)
5. [Workflow CI/CD](#workflow-cicd)
6. [Principes de Priorisation et Planification Data](#principes-de-priorisation-et-planification-data)
7. [Guide de DÃ©marrage Rapide](#guide-de-dÃ©marrage-rapide)
8. [Maintenance et Exploitation](#maintenance-et-exploitation)

---

## Vue d'ensemble

### Contexte MÃ©tier

L'entreprise dispose de **deux sources de donnÃ©es magasins** distinctes dans l'environnement Snowflake `DTL_EXO`:

- **TH.magasins**: 186,992 magasins (coordonnÃ©es GPS en FLOAT)
- **GI.magasins**: 33,841 magasins (coordonnÃ©es GPS en NUMBER)

Ces sources sont **mises Ã  jour rÃ©guliÃ¨rement** et contiennent des informations susceptibles de se recouper.

### Objectif du Projet

Construire une **dimension magasin unique et historisÃ©e** (SCD Type 2) dans le Data Warehouse avec:

1. **Consolidation** des deux sources en un rÃ©fÃ©rentiel unique
2. â­ **Validation GPS** : Correction automatique des coordonnÃ©es erronÃ©es via rÃ©fÃ©rentiel INSEE (80.9% stores avec anomalies)
3. â­ **DÃ©doublonnage** : Fuzzy matching intelligent (similaritÃ© nom + distance GPS) avec prÃ©-filtrage par commune
4. **Enrichissement gÃ©ographique** via le rÃ©fÃ©rentiel communes France 2025 (data.gouv.fr)
5. **Historisation** complÃ¨te des changements (SCD Type 2)
6. **Workflow DataOps** automatisÃ© (DEV â†’ PROD avec validation)
7. **Gouvernance RBAC** stricte (principe du moindre privilÃ¨ge)
8. **ObservabilitÃ©** et qualitÃ© des donnÃ©es intÃ©grÃ©es

### Environnements Snowflake

| Environnement | Database | Usage | AccÃ¨s |
|---------------|----------|-------|-------|
| **Source** | `DTL_EXO` | Tables brutes TH/GI | Read-only |
| **DÃ©veloppement** | `DWH_DEV_ABDELFATTAH` | Tests et itÃ©rations | Data Engineers (RW) |
| **Production** | `DWH_PROD_ABDELFATTAH` | DonnÃ©es validÃ©es | CI/CD only (via approval) |

---

## Architecture Technique

### Diagramme d'Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DTL_EXO (Sources)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TH.magasins     â”‚  GI.magasins      â”‚  (Read-only)             â”‚
â”‚  186,992 stores  â”‚  33,841 stores    â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   dbt Staging Layer  â”‚
                            â”‚  stg_th_magasins     â”‚
                            â”‚  stg_gi_magasins     â”‚
                            â”‚  (+ quality flags)   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ dbt Intermediate        â”‚
                            â”‚ int_magasins_merged     â”‚
                            â”‚ int_magasins_geo_       â”‚
                            â”‚   validated (â­ NEW)    â”‚
                            â”‚ int_magasins_fuzzy_     â”‚
                            â”‚   dedup                 â”‚
                            â”‚ int_magasins_augmented  â”‚
                            â”‚ (+ GPS correction +     â”‚
                            â”‚  deduplication)         â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”
                    â”‚       dbt Marts Layer              â”‚
                    â”‚    dim_magasin (SCD Type 2)        â”‚
                    â”‚  215,828 stores enrichis           â”‚
                    â”‚  valid_from | valid_to | is_currentâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜
                                      â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Observability Layer â”‚
                            â”‚  data_quality        â”‚
                            â”‚  freshness           â”‚
                            â”‚  anomalies           â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture DataOps

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Developer Workflow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Feature Branch â”‚
                   â”‚  (Git)          â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Pull Request   â”‚
                   â”‚  to develop     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     GitHub Actions CI/CD              â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  1. Lint (sqlfluff)                   â”‚
        â”‚  2. Test on DEV (dbt test)            â”‚
        â”‚  3. Merge to develop â†’ Auto deploy DEVâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Merge develop  â”‚
                   â”‚  to main        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  GitHub Actions - PROD Deployment     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  1. Flyway Migrations (DDL)           â”‚
        â”‚  2. Manual Approval Required âœ‹       â”‚
        â”‚  3. dbt run --target prod             â”‚
        â”‚  4. dbt test --target prod --fail-fastâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  DWH_PROD       â”‚
                   â”‚  (Production)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack Technologique

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **Transformation** | dbt 1.10.13 | ELT, modÃ©lisation dimensionnelle |
| **Data Warehouse** | Snowflake | Stockage et compute |
| **DDL Migrations** | Flyway | Versionnement schÃ©mas, RBAC |
| **CI/CD** | GitHub Actions | Automatisation lint/test/deploy |
| **Versioning** | Git | ContrÃ´le de version, branches |
| **Linting** | sqlfluff | Validation syntaxe SQL |
| **Tests** | dbt tests + dbt_expectations | QualitÃ© donnÃ©es |

### Structure du Projet

```
gg_vp_data/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci_cd.yml              # Pipeline CI/CD complet (lint â†’ test â†’ deploy)
â”‚
â”œâ”€â”€ dbt_project.yml                # Configuration dbt (matÃ©rialisations, schÃ©mas)
â”œâ”€â”€ packages.yml                   # DÃ©pendances (dbt_utils, dbt_expectations)
â”‚
â”œâ”€â”€ flyway/                        # Migrations DDL versionnÃ©es (RBAC + schÃ©mas)
â”‚   â”œâ”€â”€ flyway.conf                # Config Flyway (connexion Snowflake)
â”‚   â””â”€â”€ sql/
â”‚       â”œâ”€â”€ V001__create_databases_schemas.sql
â”‚       â”œâ”€â”€ V002__create_warehouses.sql
â”‚       â”œâ”€â”€ V003__create_roles.sql
â”‚       â””â”€â”€ V004__grant_permissions.sql
â”‚
â”œâ”€â”€ models/                        # ModÃ¨les dbt (transformations SQL)
â”‚   â”œâ”€â”€ sources.yml                # DÃ©claration sources + freshness monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ staging/                   # Couche 1: Nettoyage + typage
â”‚   â”‚   â”œâ”€â”€ staging.yml            # Tests sources
â”‚   â”‚   â”œâ”€â”€ stg_th_magasins.sql    # Source TH (186k stores)
â”‚   â”‚   â””â”€â”€ stg_gi_magasins.sql    # Source GI (34k stores)
â”‚   â”‚
â”‚   â”œâ”€â”€ intermediate/              # Couche 2: Transformations mÃ©tier
â”‚   â”‚   â”œâ”€â”€ intermediate.yml       # Tests intermÃ©diaires
â”‚   â”‚   â”œâ”€â”€ int_magasins_merged.sql          # Union TH + GI
â”‚   â”‚   â”œâ”€â”€ int_magasins_geo_validated.sql   # â­ GPS validation INSEE
â”‚   â”‚   â”œâ”€â”€ int_magasins_fuzzy_dedup.sql     # â­ DÃ©doublonnage fuzzy
â”‚   â”‚   â””â”€â”€ int_magasins_augmented.sql       # Enrichissement gÃ©o
â”‚   â”‚
â”‚   â”œâ”€â”€ marts/                     # Couche 3: Tables exposÃ©es (dimensions/faits)
â”‚   â”‚   â”œâ”€â”€ marts.yml              # Tests + documentation SCD2
â”‚   â”‚   â””â”€â”€ dim_magasin.sql        # Dimension magasin (SCD Type 2)
â”‚   â”‚
â”‚   â””â”€â”€ observability/             # MÃ©triques qualitÃ© donnÃ©es
â”‚       â”œâ”€â”€ observability.yml      # Tests observabilitÃ©
â”‚       â”œâ”€â”€ observability__data_quality.sql   # ComplÃ©tude, cohÃ©rence
â”‚       â”œâ”€â”€ observability__freshness.sql      # SLA fraÃ®cheur <24h
â”‚       â””â”€â”€ observability__anomalies.sql      # DÃ©tection anomalies
â”‚
â”œâ”€â”€ macros/                        # Fonctions SQL rÃ©utilisables (Jinja)
â”‚   â”œâ”€â”€ haversine_distance.sql     # Distance GPS (formule Haversine)
â”‚   â”œâ”€â”€ text_similarity.sql        # SimilaritÃ© texte (EDITDISTANCE normalisÃ©)
â”‚   â”œâ”€â”€ extract_city_from_name.sql # â­ Extraction ville par regex
â”‚   â”œâ”€â”€ generate_schema_name.sql   # Override schÃ©ma naming dbt
â”‚   â”œâ”€â”€ drop_table.sql             # Utility drop table safe
â”‚   â””â”€â”€ show_tables.sql            # Utility list tables
â”‚
â”œâ”€â”€ analyses/                      # RequÃªtes SQL ad-hoc (analytics exploratoires)
â”‚   â”œâ”€â”€ dedup_metrics.sql          # â­ MÃ©triques dÃ©doublonnage (golden records)
â”‚   â”œâ”€â”€ gps_correction_impact.sql  # â­ Impact correction GPS INSEE
â”‚   â””â”€â”€ query_results.sql          # â­ RÃ©sultats aggregÃ©s finaux
â”‚
â”œâ”€â”€ tests/                         # Tests custom mÃ©tier (dbt singular tests)
â”‚   â”œâ”€â”€ assert_scd2_one_current_per_store.sql  # UnicitÃ© is_current=TRUE
â”‚   â”œâ”€â”€ assert_all_sources_in_dim.sql          # ExhaustivitÃ© TH+GIâ†’dim
â”‚   â””â”€â”€ assert_scd2_no_overlapping_dates.sql   # IntÃ©gritÃ© pÃ©riodes SCD2
â”‚
â”œâ”€â”€ seeds/                         # DonnÃ©es rÃ©fÃ©rentielles (CSV statiques)
â”‚   â”œâ”€â”€ communes-france-2025.csv   # RÃ©fÃ©rentiel INSEE (34k communes, 2MB)
â”‚   â””â”€â”€ communes-france-2025.yml   # Config + tests seed
â”‚
â”œâ”€â”€ docs/                          # Documentation architecture & dÃ©cisions
â”‚   â”œâ”€â”€ README.md                  # Index documentation (parcours utilisateurs)
â”‚   â”œâ”€â”€ FAQ.md                     # 40+ questions/rÃ©ponses
â”‚   â”œâ”€â”€ ADR-001-scd-type-2.md      # DÃ©cision SCD Type 2
â”‚   â”œâ”€â”€ ADR-002-matching-strategy.md # DÃ©cision matching fuzzy
â”‚   â”œâ”€â”€ ADR-003-deduplication-fuzzy-matching.md  # Algorithme dÃ©doublonnage
â”‚   â”œâ”€â”€ ADR-004-gps-validation-correction.md     # â­ Validation GPS INSEE
â”‚   â”œâ”€â”€ ROADMAP.md                 # Ã‰volutions futures
â”‚   â””â”€â”€ AUDIT_DOCUMENTATION.md     # Bilan audit documentation
â”‚
â”œâ”€â”€ images/                        # Captures d'Ã©cran dbt docs
â”‚   â”œâ”€â”€ lineage.png                # Graphe dÃ©pendances modÃ¨les
â”‚   â””â”€â”€ details.png                # DÃ©tails table dim_magasin
â”‚
â”œâ”€â”€ CHANGELOG.md                   # Historique versions (Keep a Changelog)
â””â”€â”€ README.md                      # ğŸ‘ˆ Ce fichier (guide complet)
```

**LÃ©gende** :
- ğŸ“ **staging** : Nettoyage sources (typage, normalisation)
- ğŸ”§ **intermediate** : Transformations mÃ©tier (GPS, dedup, enrichissement)
- ğŸ“Š **marts** : Tables exposÃ©es (dimensions/faits pour BI)
- ğŸ” **observability** : MÃ©triques qualitÃ© temps rÃ©el
- â­ **NEW** : AjoutÃ© dans version 1.2.0 (2025-11-08)

### ModÃ¨le de DonnÃ©es

#### SCD Type 2: dim_magasin

**ClÃ© primaire:** `magasin_key` (MD5 surrogate key)
**ClÃ© mÃ©tier:** `(magasin_id, source_system)`
**GranularitÃ©:** Un enregistrement par version de magasin

| Colonne | Type | Description | SCD |
|---------|------|-------------|-----|
| `magasin_key` | VARCHAR | ClÃ© surrogate unique | - |
| `magasin_id` | NUMBER | ID mÃ©tier du magasin | Business key |
| `nom_magasin` | VARCHAR | Nom du magasin | Tracked |
| `latitude` | FLOAT | Latitude GPS (WGS84) | Tracked |
| `longitude` | FLOAT | Longitude GPS (WGS84) | Tracked |
| `source_system` | VARCHAR | 'TH' ou 'GI' | Business key |
| `commune_nom` | VARCHAR | Commune enrichie | - |
| `code_postal` | VARCHAR | Code postal | - |
| `dep_nom` | VARCHAR | DÃ©partement | - |
| `reg_nom` | VARCHAR | RÃ©gion | - |
| `coords_dans_plage_france` | BOOLEAN | GPS dans plage France | Tracked |
| `match_fiable` | BOOLEAN | Matching fiable (>70% sim, <50km) | - |
| `coords_correction_requise` | BOOLEAN | Distance > 50km | - |
| **`valid_from`** | **TIMESTAMP** | **Date dÃ©but validitÃ©** | **SCD** |
| **`valid_to`** | **TIMESTAMP** | **Date fin validitÃ© (NULL si current)** | **SCD** |
| **`is_current`** | **BOOLEAN** | **TRUE = version actuelle** | **SCD** |

**CritÃ¨res de dÃ©tection de changement:**
- Nom du magasin change
- Latitude change
- Longitude change
- Flag `coords_dans_plage_france` change

---

## Politique de Gouvernance et SÃ©curitÃ© (RBAC)

### Principes de SÃ©curitÃ©

1. **Least Privilege (Moindre PrivilÃ¨ge)**
   - Chaque rÃ´le a le minimum de permissions nÃ©cessaires
   - SÃ©paration lecture/Ã©criture stricte
   - Pas d'accÃ¨s direct PROD pour les humains

2. **Separation of Duties (SÃ©paration des ResponsabilitÃ©s)**
   - Flyway = DDL (CREATE, ALTER, DROP)
   - dbt = DML (INSERT, UPDATE, DELETE via transformations)
   - Analystes = Lecture seule (SELECT)

3. **Defense in Depth (DÃ©fense en Profondeur)**
   - Isolation par warehouse
   - Permissions par schÃ©ma
   - Permissions par table
   - FUTURE GRANTS pour nouveaux objets

4. **Audit Trail (TraÃ§abilitÃ©)**
   - GitHub Actions logs toutes les actions CI/CD
   - Snowflake QUERY_HISTORY track toutes les requÃªtes
   - Flyway track toutes les migrations DDL

### RÃ´les et Permissions

#### RÃ´les Fonctionnels (Humains)

| RÃ´le | Profil | AccÃ¨s Warehouses | AccÃ¨s SchÃ©mas | PrivilÃ¨ges |
|------|--------|------------------|---------------|------------|
| **DATA_ENGINEER** | IngÃ©nieurs data | TRANSFORM_WH, ANALYTICS_WH, ADMIN_WH | DEV: ALL, PROD: READ | CREATE, ALTER, DROP (DEV only), SELECT |
| **DATA_ANALYST** | Analystes mÃ©tier | ANALYTICS_WH | marts, observability (RO) | SELECT |
| **PRODUCT_OWNER** | Product Owners | ANALYTICS_WH | marts, observability (RO) | SELECT |
| **CLIENT_VIEWER** | Clients externes | CLIENT_WH | marts (RO) - vues limitÃ©es | SELECT (avec row-level security future) |

#### RÃ´les Techniques (Service Accounts)

| RÃ´le | Usage | AccÃ¨s Warehouses | PrivilÃ¨ges |
|------|-------|------------------|------------|
| **DBT_RUNNER** | Transformations dbt | TRANSFORM_WH | CREATE, INSERT, UPDATE, DELETE (staging, intermediate, marts) |
| **FLYWAY_DEPLOYER** | Migrations DDL | ADMIN_WH | CREATE SCHEMA, ALTER, DROP, GRANT |
| **CICD_PIPELINE** | GitHub Actions | TRANSFORM_WH, ADMIN_WH | HÃ©rite DBT_RUNNER + FLYWAY_DEPLOYER |

**C'est quoi ?**

Les **Service Accounts** (comptes techniques) sont des **utilisateurs non-humains** crÃ©Ã©s spÃ©cifiquement pour l'automatisation. Contrairement aux comptes personnels (`alice_analyst`, `bob_engineer`), ils sont utilisÃ©s par des **systÃ¨mes automatisÃ©s** (CI/CD, orchestrateurs, scripts).

**CaractÃ©ristiques** :
- âŒ Pas de login interactif humain
- âœ… Identifiants stockÃ©s dans secrets management (GitHub Secrets, Vault)
- âœ… Permissions minimales strictes (principe du moindre privilÃ¨ge)
- âœ… TraÃ§abilitÃ© complÃ¨te via logs Snowflake QUERY_HISTORY

**Pourquoi faire ?**

1. **SÃ©curitÃ©** :
   - Ã‰viter de partager credentials personnels dans CI/CD
   - Rotation facile des mots de passe (sans impacter dÃ©veloppeurs)
   - Audit trail clair : `github_actions_bot` vs `bob_engineer`

2. **SÃ©paration des responsabilitÃ©s** :
   - `DBT_RUNNER` = Transformations DML uniquement
   - `FLYWAY_DEPLOYER` = Migrations DDL uniquement
   - `CICD_PIPELINE` = Orchestration (hÃ©rite les 2)

3. **ReproductibilitÃ©** :
   - DÃ©ploiements identiques en DEV/PROD
   - Pas de dÃ©pendance Ã  un dÃ©veloppeur spÃ©cifique

**Comment ?**

**1. CrÃ©ation du Service Account dans Snowflake :**
```sql
-- CrÃ©er utilisateur technique
CREATE USER github_actions_bot
  PASSWORD = 'SECURE_RANDOM_PASSWORD_32_CHARS'
  DEFAULT_ROLE = CICD_PIPELINE
  COMMENT = 'Service account for GitHub Actions CI/CD';

-- Assigner rÃ´le
GRANT ROLE CICD_PIPELINE TO USER github_actions_bot;

-- DÃ©sactiver login interactif (optionnel mais recommandÃ©)
ALTER USER github_actions_bot SET MUST_CHANGE_PASSWORD = FALSE;
```

**2. Configuration dans GitHub Actions :**
```yaml
# .github/workflows/ci_cd.yml
jobs:
  deploy-prod:
    steps:
      - name: dbt run
        env:
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_PROD_USER }}      # = github_actions_bot
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PROD_PASSWORD }}
          SNOWFLAKE_ROLE: CICD_PIPELINE
        run: dbt run --target prod
```

**3. TraÃ§abilitÃ© :**
```sql
-- VÃ©rifier actions du service account
SELECT
  query_text,
  user_name,
  role_name,
  execution_status,
  start_time
FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
WHERE user_name = 'GITHUB_ACTIONS_BOT'
  AND start_time > DATEADD(day, -7, CURRENT_TIMESTAMP())
ORDER BY start_time DESC;
```

#### HiÃ©rarchie des RÃ´les

```
ACCOUNTADMIN
    â”œâ”€â”€ SYSADMIN
    â”‚   â”œâ”€â”€ DATA_ENGINEER
    â”‚   â”‚   â”œâ”€â”€ DATA_ANALYST
    â”‚   â”‚   â””â”€â”€ DBT_RUNNER
    â”‚   â”œâ”€â”€ PRODUCT_OWNER
    â”‚   â”œâ”€â”€ CLIENT_VIEWER
    â”‚   â”œâ”€â”€ FLYWAY_DEPLOYER
    â”‚   â””â”€â”€ CICD_PIPELINE
    â”‚       â”œâ”€â”€ DBT_RUNNER (hÃ©ritÃ©)
    â”‚       â””â”€â”€ FLYWAY_DEPLOYER (hÃ©ritÃ©)
    â””â”€â”€ SECURITYADMIN
```

### Warehouses DÃ©diÃ©s

| Warehouse | Taille | Auto-Suspend | Usage | RÃ´les AutorisÃ©s |
|-----------|--------|--------------|-------|-----------------|
| **TRANSFORM_WH** | MEDIUM | 60s | dbt transformations ETL | DATA_ENGINEER, DBT_RUNNER, CICD_PIPELINE |
| **ANALYTICS_WH** | SMALL | 120s | RequÃªtes analystes, reporting | DATA_ENGINEER, DATA_ANALYST, PRODUCT_OWNER |
| **CLIENT_WH** | XSMALL | 180s | AccÃ¨s clients externes (RO) | CLIENT_VIEWER |
| **ADMIN_WH** | XSMALL | 300s | TÃ¢ches admin, Flyway migrations | DATA_ENGINEER, FLYWAY_DEPLOYER, CICD_PIPELINE |

**C'est quoi ?**

Les **Warehouses** (entrepÃ´ts de calcul) dans Snowflake sont des **clusters de compute sÃ©parÃ©s** qui exÃ©cutent les requÃªtes. Chaque warehouse est **isolÃ©** et possÃ¨de ses propres ressources CPU/mÃ©moire. C'est l'Ã©quivalent d'un "moteur de requÃªtes" dÃ©diÃ©.

**CaractÃ©ristiques** :
- âœ… **Isolation totale** : Un warehouse surchargÃ© n'impacte pas les autres
- âœ… **Facturation sÃ©parÃ©e** : CoÃ»ts trackables par usage (ETL vs Analytics vs Admin)
- âœ… **Auto-suspend** : ArrÃªt automatique aprÃ¨s inactivitÃ© (Ã©conomies)
- âœ… **Tailles variables** : XSMALL â†’ 4X-LARGE (scaling vertical)

**Pourquoi faire ?**

1. **Isolation des charges de travail** :
   - ETL lourd (dbt CROSS JOIN) sur `TRANSFORM_WH` â†’ N'impacte pas analystes sur `ANALYTICS_WH`
   - Dashboard client lent â†’ N'impacte pas production sur `TRANSFORM_WH`

2. **Optimisation des coÃ»ts** :
   - `ANALYTICS_WH` (SMALL, 120s) : Usage sporadique analystes â†’ Suspend rapidement
   - `TRANSFORM_WH` (MEDIUM, 60s) : ETL intensif â†’ Plus de compute, suspend vite
   - `CLIENT_WH` (XSMALL, 180s) : Lectures simples â†’ Minimal compute, suspend lentement

3. **SÃ©curitÃ© et traÃ§abilitÃ©** :
   - Logs Snowflake montrent quel rÃ´le utilise quel warehouse
   - DÃ©tection anomalies (ex: `CLIENT_VIEWER` utilise `ADMIN_WH` = alerte sÃ©curitÃ©)

4. **Performance prÃ©visible** :
   - Analystes ont ressources garanties (pas de contention avec ETL)

**Comment ?**

**1. CrÃ©ation des Warehouses dans Snowflake :**
```sql
-- Warehouse pour transformations dbt
CREATE WAREHOUSE TRANSFORM_WH
  WAREHOUSE_SIZE = 'MEDIUM'
  AUTO_SUSPEND = 60
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Dedicated compute for dbt ETL transformations';

-- Warehouse pour analystes
CREATE WAREHOUSE ANALYTICS_WH
  WAREHOUSE_SIZE = 'SMALL'
  AUTO_SUSPEND = 120
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Dedicated compute for analyst queries and reporting';

-- Warehouse pour clients (read-only)
CREATE WAREHOUSE CLIENT_WH
  WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 180
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Dedicated compute for external client access';

-- Warehouse pour admin
CREATE WAREHOUSE ADMIN_WH
  WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 300
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Dedicated compute for admin tasks and Flyway migrations';
```

**2. Attribution des Permissions :**
```sql
-- DATA_ENGINEER peut utiliser TRANSFORM_WH et ANALYTICS_WH
GRANT USAGE ON WAREHOUSE TRANSFORM_WH TO ROLE DATA_ENGINEER;
GRANT USAGE ON WAREHOUSE ANALYTICS_WH TO ROLE DATA_ENGINEER;

-- DBT_RUNNER (CI/CD) uniquement TRANSFORM_WH
GRANT USAGE ON WAREHOUSE TRANSFORM_WH TO ROLE DBT_RUNNER;

-- DATA_ANALYST uniquement ANALYTICS_WH
GRANT USAGE ON WAREHOUSE ANALYTICS_WH TO ROLE DATA_ANALYST;
```

**3. Utilisation dans dbt profiles.yml :**
```yaml
gg_vp_data:
  outputs:
    dev:
      warehouse: TRANSFORM_WH  # â† SpÃ©cifie quel warehouse utiliser
    prod:
      warehouse: TRANSFORM_WH
```

**4. Monitoring des CoÃ»ts :**
```sql
-- CoÃ»t par warehouse (derniers 7 jours)
SELECT
  warehouse_name,
  SUM(credits_used) AS total_credits,
  SUM(credits_used) * 3.0 AS estimated_cost_usd  -- Tarif approximatif Snowflake
FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
WHERE start_time > DATEADD(day, -7, CURRENT_TIMESTAMP())
GROUP BY warehouse_name
ORDER BY total_credits DESC;
```

**RÃ©sultat Monitoring (exemple)** :
| Warehouse | Credits (7j) | CoÃ»t EstimÃ© | Utilisation |
|-----------|--------------|-------------|-------------|
| TRANSFORM_WH | 12.5 | $37.50 | ETL quotidien (dbt run ~20 min/jour) |
| ANALYTICS_WH | 2.3 | $6.90 | RequÃªtes analystes (sporadique) |
| ADMIN_WH | 0.8 | $2.40 | Migrations Flyway (rare) |
| CLIENT_WH | 0.1 | $0.30 | Dashboards clients (faible usage) |

### Matrice d'AccÃ¨s DÃ©taillÃ©e

#### DEV Environment (DWH_DEV_ABDELFATTAH)

| SchÃ©ma | DATA_ENGINEER | DATA_ANALYST | DBT_RUNNER | FLYWAY_DEPLOYER |
|--------|---------------|--------------|------------|-----------------|
| staging | ALL | - | ALL | ALL |
| intermediate | ALL | - | ALL | ALL |
| marts | ALL | SELECT | ALL | ALL |
| observability | ALL | SELECT | SELECT | ALL |

#### PROD Environment (DWH_PROD_ABDELFATTAH)

| SchÃ©ma | DATA_ENGINEER | DATA_ANALYST | PRODUCT_OWNER | CLIENT_VIEWER | DBT_RUNNER | FLYWAY_DEPLOYER |
|--------|---------------|--------------|---------------|---------------|------------|-----------------|
| staging | SELECT | - | - | - | ALL (via CI/CD) | ALL |
| intermediate | SELECT | - | - | - | ALL (via CI/CD) | ALL |
| marts | SELECT | SELECT | SELECT | SELECT | ALL (via CI/CD) | ALL |
| observability | SELECT | SELECT | SELECT | - | SELECT | ALL |

**âš ï¸ Point Critique:** Aucun humain n'a d'accÃ¨s Ã©criture direct en PROD. Tous les dÃ©ploiements passent par CI/CD avec approbation manuelle.

**C'est quoi ?**

La **Matrice d'AccÃ¨s** est un tableau de rÃ©fÃ©rence qui spÃ©cifie **exactement** qui peut faire quoi sur chaque schÃ©ma dans chaque environnement. C'est le contrat de sÃ©curitÃ© du projet.

**Principes** :
- âœ… **Least Privilege** : Minimum requis pour accomplir sa mission
- âœ… **Separation of Duties** : RÃ´les techniques vs fonctionnels sÃ©parÃ©s
- âœ… **Environment Isolation** : DEV = permissif, PROD = restrictif
- âœ… **Zero Trust** : Aucun accÃ¨s par dÃ©faut, tout doit Ãªtre explicite

**Comment c'est assurÃ© ?**

L'assurance de cette matrice repose sur **4 niveaux de contrÃ´le** :

**1. ContrÃ´le au DÃ©ploiement (Flyway Migrations VersionnÃ©es)**

Toutes les permissions sont dÃ©finies dans des migrations SQL versionnÃ©es et auditables :

```sql
-- flyway/sql/V004__grant_permissions.sql

-- ============================================
-- DEV ENVIRONMENT: Permissions permissives
-- ============================================

-- DATA_ENGINEER: Full access en DEV
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;

-- DATA_ANALYST: Read-only marts en DEV
GRANT USAGE ON SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;

-- ============================================
-- PROD ENVIRONMENT: Permissions restrictives
-- ============================================

-- DATA_ENGINEER: READ-ONLY en PROD (pas de write direct!)
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;

GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;

-- DBT_RUNNER: Full write access (via CI/CD uniquement)
GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DBT_RUNNER;
GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;
GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DBT_RUNNER;

-- Assurer que les futures tables hÃ©ritent des permissions
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
```

**Avantages Flyway** :
- âœ… Versionnage Git (audit trail complet)
- âœ… Idempotence (re-run safe)
- âœ… Rollback possible (si Flyway Teams)
- âœ… Validation CI/CD avant dÃ©ploiement

**2. ContrÃ´le Ã  l'ExÃ©cution (Snowflake RBAC Natif)**

Snowflake enforce les permissions au niveau systÃ¨me :

```sql
-- VÃ©rifier permissions effectives d'un rÃ´le
SHOW GRANTS TO ROLE DATA_ENGINEER;
```

Exemple sortie:

| privilege | granted_on | name | granted_to | granted_by |
|-----------|------------|------|------------|------------|
| ALL       | SCHEMA     | DWH_DEV_ABDELFATTAH.staging | ROLE DATA_ENGINEER | SECURITYADMIN |
| SELECT    | SCHEMA     | DWH_PROD_ABDELFATTAH.marts  | ROLE DATA_ENGINEER | SECURITYADMIN |


**3. ContrÃ´le d'Authentification (Service Accounts DÃ©diÃ©s)**

En PROD, **seuls les service accounts** peuvent modifier les donnÃ©es :

```yaml
# .github/workflows/ci_cd.yml (PROD deployment)
deploy-prod:
  environment: production  # â† Requiert approval manuelle
  steps:
    - name: dbt run PROD
      env:
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_PROD_USER }}      # = github_actions_bot
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PROD_PASSWORD }}
        SNOWFLAKE_ROLE: DBT_RUNNER  # â† Pas DATA_ENGINEER!
```

**Humains en PROD** :
```bash
# Bob (DATA_ENGINEER) se connecte en PROD
snowsql -a ACCOUNT -u bob_engineer -r DATA_ENGINEER

# Tentative modification â†’ Ã‰CHOUE
bob_engineer@DWH_PROD> DELETE FROM marts.dim_magasin WHERE is_current = FALSE;
-- âŒ SQL access control error: Insufficient privileges to operate on table 'DIM_MAGASIN'
```

**4. ContrÃ´le Post-DÃ©ploiement (Tests AutomatisÃ©s)**

Tests dbt vÃ©rifient l'intÃ©gritÃ© des permissions :

```sql
-- tests/assert_prod_permissions_read_only.sql (custom test)
-- VÃ©rifier qu'aucun humain n'a de privilÃ¨ge write en PROD

SELECT
  r.name AS role_name,
  p.privilege
FROM SNOWFLAKE.ACCOUNT_USAGE.GRANTS_TO_ROLES p
JOIN SNOWFLAKE.ACCOUNT_USAGE.ROLES r ON p.grantee_name = r.name
WHERE p.table_schema LIKE 'DWH_PROD%'
  AND p.privilege IN ('INSERT', 'UPDATE', 'DELETE', 'TRUNCATE')
  AND r.name NOT IN ('DBT_RUNNER', 'FLYWAY_DEPLOYER', 'CICD_PIPELINE')  -- Exclus service accounts
HAVING COUNT(*) > 0;  -- âŒ Test Ã©choue si rÃ©sultat non vide
```

**Audit Continu** :
```sql
-- Qui a modifiÃ© quoi en PROD (derniÃ¨res 24h)
SELECT
  user_name,
  role_name,
  query_type,
  query_text,
  execution_status,
  start_time
FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
WHERE database_name = 'DWH_PROD_ABDELFATTAH'
  AND query_type IN ('INSERT', 'UPDATE', 'DELETE', 'MERGE')
  AND start_time > DATEADD(hour, -24, CURRENT_TIMESTAMP())
ORDER BY start_time DESC;
```

**RÃ©sumÃ© des Assurances** :

| Niveau | MÃ©canisme | FrÃ©quence | Responsable |
|--------|-----------|-----------|-------------|
| **DÃ©ploiement** | Flyway migrations SQL | Ã€ chaque deploy | CICD_PIPELINE |
| **Runtime** | Snowflake RBAC natif | Chaque requÃªte | Snowflake |
| **Authentification** | Service accounts (PROD) | Chaque connexion | GitHub Actions |
| **Audit** | Tests dbt + QUERY_HISTORY | Post-deploy + continu | Data Engineers |

### ImplÃ©mentation RBAC

Les rÃ´les et permissions sont crÃ©Ã©s via **Flyway migrations versionnÃ©es**. Voici un exemple **complet et rÃ©aliste** couvrant tout le cycle de vie RBAC.

#### Ã‰tape 1: CrÃ©ation de la HiÃ©rarchie de RÃ´les

```sql
-- flyway/sql/V003__create_roles.sql
-- ExÃ©cutÃ© par: SECURITYADMIN (via Flyway ADMIN_WH)
-- Date: 2025-01-15 (Version 1.0 du projet)

USE ROLE SECURITYADMIN;

-- ============================================
-- RÃ”LES FONCTIONNELS (Humains)
-- ============================================

-- RÃ´le pour ingÃ©nieurs data (full access DEV, read-only PROD)
CREATE ROLE IF NOT EXISTS DATA_ENGINEER
  COMMENT = 'Data engineers with dev access and prod read-only';

-- RÃ´le pour analystes mÃ©tier (read-only marts uniquement)
CREATE ROLE IF NOT EXISTS DATA_ANALYST
  COMMENT = 'Business analysts with read-only access to marts layer';

-- RÃ´le pour product owners (read-only marts + observability)
CREATE ROLE IF NOT EXISTS PRODUCT_OWNER
  COMMENT = 'Product owners with read-only access to marts and observability';

-- RÃ´le pour clients externes (read-only marts via vues filtrÃ©es)
CREATE ROLE IF NOT EXISTS CLIENT_VIEWER
  COMMENT = 'External clients with restricted read-only access';

-- ============================================
-- RÃ”LES TECHNIQUES (Service Accounts)
-- ============================================

-- RÃ´le pour exÃ©cution dbt (transformations DML)
CREATE ROLE IF NOT EXISTS DBT_RUNNER
  COMMENT = 'Service account for dbt transformations (DML only)';

-- RÃ´le pour dÃ©ploiements Flyway (migrations DDL)
CREATE ROLE IF NOT EXISTS FLYWAY_DEPLOYER
  COMMENT = 'Service account for Flyway migrations (DDL only)';

-- RÃ´le CI/CD qui hÃ©rite DBT_RUNNER + FLYWAY_DEPLOYER
CREATE ROLE IF NOT EXISTS CICD_PIPELINE
  COMMENT = 'GitHub Actions service account (orchestration)';

-- ============================================
-- HIÃ‰RARCHIE DES RÃ”LES
-- ============================================

-- DATA_ANALYST hÃ©rite par DATA_ENGINEER (engineers peuvent tester en tant qu'analyst)
GRANT ROLE DATA_ANALYST TO ROLE DATA_ENGINEER;

-- CICD_PIPELINE hÃ©rite DBT_RUNNER + FLYWAY_DEPLOYER
GRANT ROLE DBT_RUNNER TO ROLE CICD_PIPELINE;
GRANT ROLE FLYWAY_DEPLOYER TO ROLE CICD_PIPELINE;

-- Tous les rÃ´les hÃ©ritent par SYSADMIN (pour gestion centralisÃ©e)
GRANT ROLE DATA_ENGINEER TO ROLE SYSADMIN;
GRANT ROLE DATA_ANALYST TO ROLE SYSADMIN;
GRANT ROLE PRODUCT_OWNER TO ROLE SYSADMIN;
GRANT ROLE CLIENT_VIEWER TO ROLE SYSADMIN;
GRANT ROLE DBT_RUNNER TO ROLE SYSADMIN;
GRANT ROLE FLYWAY_DEPLOYER TO ROLE SYSADMIN;
GRANT ROLE CICD_PIPELINE TO ROLE SYSADMIN;
```

#### Ã‰tape 2: Attribution des Permissions sur Warehouses

```sql
-- flyway/sql/V004__grant_warehouse_permissions.sql

USE ROLE SECURITYADMIN;

-- ============================================
-- TRANSFORM_WH (ETL dbt)
-- ============================================
GRANT USAGE ON WAREHOUSE TRANSFORM_WH TO ROLE DATA_ENGINEER;
GRANT USAGE ON WAREHOUSE TRANSFORM_WH TO ROLE DBT_RUNNER;
GRANT USAGE ON WAREHOUSE TRANSFORM_WH TO ROLE CICD_PIPELINE;

-- ============================================
-- ANALYTICS_WH (RequÃªtes analystes)
-- ============================================
GRANT USAGE ON WAREHOUSE ANALYTICS_WH TO ROLE DATA_ENGINEER;
GRANT USAGE ON WAREHOUSE ANALYTICS_WH TO ROLE DATA_ANALYST;
GRANT USAGE ON WAREHOUSE ANALYTICS_WH TO ROLE PRODUCT_OWNER;

-- ============================================
-- CLIENT_WH (Dashboards clients externes)
-- ============================================
GRANT USAGE ON WAREHOUSE CLIENT_WH TO ROLE CLIENT_VIEWER;

-- ============================================
-- ADMIN_WH (Migrations Flyway + admin tasks)
-- ============================================
GRANT USAGE ON WAREHOUSE ADMIN_WH TO ROLE DATA_ENGINEER;
GRANT USAGE ON WAREHOUSE ADMIN_WH TO ROLE FLYWAY_DEPLOYER;
GRANT USAGE ON WAREHOUSE ADMIN_WH TO ROLE CICD_PIPELINE;
```

#### Ã‰tape 3: Permissions DEV Environment (Permissif)

```sql
-- flyway/sql/V005__grant_dev_permissions.sql

USE ROLE SYSADMIN;

-- ============================================
-- DATA_ENGINEER: Full access DEV (CREATE, DROP, SELECT, INSERT, etc.)
-- ============================================

-- SchÃ©ma staging
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;

-- SchÃ©ma intermediate
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;

-- SchÃ©ma marts
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;

-- SchÃ©ma observability
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;

-- ============================================
-- DBT_RUNNER: Same as DATA_ENGINEER en DEV
-- ============================================
GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.staging TO ROLE DBT_RUNNER;

GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;

GRANT ALL ON SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DBT_RUNNER;

-- ============================================
-- DATA_ANALYST: Read-only marts + observability
-- ============================================
GRANT USAGE ON SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.marts TO ROLE DATA_ANALYST;

GRANT USAGE ON SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ANALYST;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ANALYST;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_DEV_ABDELFATTAH.observability TO ROLE DATA_ANALYST;
```

#### Ã‰tape 4: Permissions PROD Environment (Restrictif)

```sql
-- flyway/sql/V006__grant_prod_permissions.sql

USE ROLE SYSADMIN;

-- ============================================
-- DATA_ENGINEER: READ-ONLY en PROD (âš ï¸ critique!)
-- ============================================

-- SchÃ©ma staging (lecture seule)
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DATA_ENGINEER;

-- SchÃ©ma intermediate (lecture seule)
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DATA_ENGINEER;

-- SchÃ©ma marts (lecture seule)
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ENGINEER;

-- SchÃ©ma observability (lecture seule)
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ENGINEER;

-- ============================================
-- DBT_RUNNER: FULL WRITE ACCESS (via CI/CD uniquement!)
-- ============================================
GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.staging TO ROLE DBT_RUNNER;

GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.intermediate TO ROLE DBT_RUNNER;

GRANT ALL ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DBT_RUNNER;
GRANT ALL ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DBT_RUNNER;
GRANT ALL ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DBT_RUNNER;

-- ============================================
-- DATA_ANALYST: Read-only marts
-- ============================================
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ANALYST;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE DATA_ANALYST;

GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ANALYST;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ANALYST;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE DATA_ANALYST;

-- ============================================
-- PRODUCT_OWNER: Read-only marts + observability
-- ============================================
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE PRODUCT_OWNER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE PRODUCT_OWNER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE PRODUCT_OWNER;

GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE PRODUCT_OWNER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE PRODUCT_OWNER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.observability TO ROLE PRODUCT_OWNER;

-- ============================================
-- CLIENT_VIEWER: Read-only marts uniquement (via vues filtrÃ©es futures)
-- ============================================
GRANT USAGE ON SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE CLIENT_VIEWER;
GRANT SELECT ON ALL TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE CLIENT_VIEWER;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DWH_PROD_ABDELFATTAH.marts TO ROLE CLIENT_VIEWER;
-- Note: En production rÃ©elle, crÃ©er des SECURE VIEWS avec row-level filtering
```

#### Ã‰tape 5: CrÃ©ation des Utilisateurs

```sql
-- flyway/sql/V007__create_users.sql

USE ROLE SECURITYADMIN;

-- ============================================
-- HUMAINS
-- ============================================

-- Data Engineer
CREATE USER IF NOT EXISTS abdelfattah_abouelaoualim
  PASSWORD = 'CHANGE_ME_SECURE_PASSWORD'
  DEFAULT_ROLE = DATA_ENGINEER
  DEFAULT_WAREHOUSE = TRANSFORM_WH
  MUST_CHANGE_PASSWORD = TRUE
  COMMENT = 'Data Engineer - Full dev access, read-only prod';

GRANT ROLE DATA_ENGINEER TO USER abdelfattah_abouelaoualim;

-- Data Analyst
CREATE USER IF NOT EXISTS alice_analyst
  PASSWORD = 'CHANGE_ME_SECURE_PASSWORD'
  DEFAULT_ROLE = DATA_ANALYST
  DEFAULT_WAREHOUSE = ANALYTICS_WH
  MUST_CHANGE_PASSWORD = TRUE
  COMMENT = 'Business Analyst - Read-only marts access';

GRANT ROLE DATA_ANALYST TO USER alice_analyst;

-- Product Owner
CREATE USER IF NOT EXISTS bob_product_owner
  PASSWORD = 'CHANGE_ME_SECURE_PASSWORD'
  DEFAULT_ROLE = PRODUCT_OWNER
  DEFAULT_WAREHOUSE = ANALYTICS_WH
  MUST_CHANGE_PASSWORD = TRUE
  COMMENT = 'Product Owner - Read-only marts + observability';

GRANT ROLE PRODUCT_OWNER TO USER bob_product_owner;

-- ============================================
-- SERVICE ACCOUNTS (Non-humains)
-- ============================================

-- Service account GitHub Actions
CREATE USER IF NOT EXISTS github_actions_bot
  PASSWORD = 'STORED_IN_GITHUB_SECRETS_VAULT'
  DEFAULT_ROLE = CICD_PIPELINE
  DEFAULT_WAREHOUSE = TRANSFORM_WH
  MUST_CHANGE_PASSWORD = FALSE  -- Service account, gÃ©rÃ© via secrets rotation
  COMMENT = 'CI/CD service account for GitHub Actions deployments';

GRANT ROLE CICD_PIPELINE TO USER github_actions_bot;
```

#### Ã‰tape 6: Validation Post-DÃ©ploiement

```sql
-- VÃ©rifier que les permissions sont correctes
USE ROLE SECURITYADMIN;

-- Test 1: DATA_ENGINEER peut lire PROD mais pas Ã©crire
SHOW GRANTS TO ROLE DATA_ENGINEER;
-- âœ… Attendu: SELECT sur DWH_PROD_ABDELFATTAH.*, pas de INSERT/UPDATE/DELETE

-- Test 2: DBT_RUNNER peut tout faire en PROD
SHOW GRANTS TO ROLE DBT_RUNNER;
-- âœ… Attendu: ALL sur DWH_PROD_ABDELFATTAH.*

-- Test 3: DATA_ANALYST ne voit que marts
SHOW GRANTS TO ROLE DATA_ANALYST;
-- âœ… Attendu: SELECT sur marts + observability, rien sur staging/intermediate

-- Test 4: VÃ©rifier hiÃ©rarchie
SELECT
  granted_to,
  grantee_name,
  name AS role_name
FROM SNOWFLAKE.ACCOUNT_USAGE.GRANTS_TO_ROLES
WHERE granted_to = 'ROLE'
  AND name IN ('DATA_ENGINEER', 'DBT_RUNNER', 'CICD_PIPELINE')
ORDER BY grantee_name;
-- âœ… Attendu: CICD_PIPELINE hÃ©rite DBT_RUNNER + FLYWAY_DEPLOYER
```

### ProcÃ©dures d'AccÃ¨s

#### Onboarding Nouvel Utilisateur

1. **Data Analyst:**
   ```sql
   CREATE USER alice_analyst PASSWORD='***' DEFAULT_ROLE=DATA_ANALYST;
   GRANT ROLE DATA_ANALYST TO USER alice_analyst;
   ```

2. **Data Engineer:**
   ```sql
   CREATE USER bob_engineer PASSWORD='***' DEFAULT_ROLE=DATA_ENGINEER;
   GRANT ROLE DATA_ENGINEER TO USER bob_engineer;
   ```

#### CrÃ©ation Service Account (CI/CD)

```sql
CREATE USER github_actions_bot PASSWORD='***' DEFAULT_ROLE=CICD_PIPELINE;
GRANT ROLE CICD_PIPELINE TO USER github_actions_bot;
```

#### Offboarding (RÃ©vocation)

```sql
REVOKE ROLE DATA_ENGINEER FROM USER bob_engineer;
ALTER USER bob_engineer SET DISABLED = TRUE;
```

---

## Gestion de la QualitÃ© et ObservabilitÃ©

### Framework de QualitÃ© des DonnÃ©es

#### Dimensions de QualitÃ© MesurÃ©es

| Dimension | Indicateur | Seuil | ImplÃ©mentation |
|-----------|-----------|-------|----------------|
| **ComplÃ©tude** | % champs non-NULL | >95% | `observability__data_quality.sql` |
| **CohÃ©rence** | % coords dans plage France | >90% | Flag `coords_dans_plage_france` |
| **FiabilitÃ©** | % matching fiable | >80% | Flag `match_fiable` (sim>0.7, dist<50km) |
| **UnicitÃ©** | 0 doublons is_current | 100% | Test custom `assert_scd2_one_current_per_store.sql` |
| **FraÃ®cheur** | DÃ©lai depuis derniÃ¨re MAJ | <24h | `observability__freshness.sql` + source freshness |

#### Tests dbt ImplÃ©mentÃ©s

**1. Tests de Base (6 tests sur sources)**
```yaml
# models/sources.yml
sources:
  - name: th
    tables:
      - name: magasins
        columns:
          - name: id
            tests:
              - not_null
              - unique
```

**2. Tests de Relations (marts.yml)**
```yaml
# models/marts/marts.yml
models:
  - name: dim_magasin
    tests:
      - dbt_expectations.expect_table_row_count_to_be_between:
          min_value: 200000
          max_value: 250000
    columns:
      - name: latitude
        tests:
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 41.0
              max_value: 51.5
```

**3. Tests Custom (3 tests mÃ©tier)**
- `tests/assert_scd2_one_current_per_store.sql`: VÃ©rifie unicitÃ© is_current=TRUE
- `tests/assert_all_sources_in_dim.sql`: VÃ©rifie exhaustivitÃ© sourceâ†’dimension
- `tests/assert_scd2_no_overlapping_dates.sql`: VÃ©rifie intÃ©gritÃ© pÃ©riodes SCD2

**4. Freshness Monitoring**
```yaml
# models/sources.yml
sources:
  - name: th
    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
```

### ObservabilitÃ© - ModÃ¨les dbt

#### 1. Data Quality (`observability__data_quality.sql`)

MÃ©triques calculÃ©es:
- Taux de complÃ©tude par colonne
- Nombre d'anomalies GPS (hors plage France)
- Taux de matching fiable
- Distribution par source (TH vs GI)

Exemple de sortie:
| metric_name | metric_value | threshold | status |
|-------------|--------------|-----------|--------|
| completeness_nom_magasin | 99.8% | 95% | âœ… OK |
| coords_france_rate | 91.2% | 90% | âœ… OK |
| match_fiable_rate | 83.5% | 80% | âœ… OK |

#### 2. Freshness (`observability__freshness.sql`)

SLA: DonnÃ©es doivent Ãªtre rafraÃ®chies **< 24h**

MÃ©triques:
- DerniÃ¨re mise Ã  jour par source
- DÃ©lai depuis derniÃ¨re exÃ©cution dbt
- Statut SLA (OK / WARN / ERROR)

#### 3. Anomalies (`observability__anomalies.sql`)

DÃ©tection automatique:
- **VolumÃ©trie**: Variation >20% du nombre de magasins
- **QualitÃ©**: Chute >10% du taux de matching fiable
- **GPS**: Pics d'anomalies gÃ©ographiques

### Alerting (Future)

IntÃ©gration prÃ©vue avec:
- **Slack**: Alertes temps rÃ©el sur Ã©checs tests
- **PagerDuty**: Incidents critiques (PROD down)
- **Datadog**: Dashboard mÃ©triques temps rÃ©el

---

## Workflow CI/CD

### Vue d'Ensemble

Le workflow CI/CD garantit que **seul du code validÃ© et testÃ©** arrive en production avec **approbation manuelle obligatoire**.

### Branches Git

| Branche | RÃ´le | Protection |
|---------|------|------------|
| `main` | Production-ready code | Require PR approval, all checks pass |
| `develop` | Integration branch | Auto-deploy to DEV |
| `feature/*` | Feature development | Delete after merge |
| `hotfix/*` | Production hotfixes | Fast-track to main |

### Pipeline CI/CD (`.github/workflows/ci_cd.yml`)

#### Trigger Events

```yaml
on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  workflow_dispatch:  # Manual trigger
```

#### Jobs SÃ©quence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 1: lint                                                â”‚
â”‚  - Checkout code                                            â”‚
â”‚  - Run sqlfluff lint models/                                â”‚
â”‚  - Comment PR with results                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (if pass)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 2: test-dev                                            â”‚
â”‚  - Setup Python + dbt                                       â”‚
â”‚  - Create profiles.yml                                      â”‚
â”‚  - dbt deps                                                 â”‚
â”‚  - dbt seed --target dev                                    â”‚
â”‚  - dbt run --target dev                                     â”‚
â”‚  - dbt test --target dev                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ If branch = develop         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 3: deploy-dev                                          â”‚
â”‚  - Run Flyway migrations (DEV)                              â”‚
â”‚  - dbt run --target dev --full-refresh                      â”‚
â”‚  - dbt test --target dev                                    â”‚
â”‚  - Generate dbt docs                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ If branch = main            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job 4: deploy-prod (REQUIRES MANUAL APPROVAL)              â”‚
â”‚  - âœ‹ Wait for manual approval in GitHub UI                 â”‚
â”‚  - Run Flyway migrations (PROD)                             â”‚
â”‚  - dbt run --target prod (incremental)                      â”‚
â”‚  - dbt test --target prod --fail-fast                       â”‚
â”‚  - Create git tag deploy-YYYYMMDD-HHMMSS                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Configuration GitHub Secrets Requise

Dans GitHub Settings â†’ Secrets â†’ Actions:

```
SNOWFLAKE_ACCOUNT=DYFAIYB-HEB08485
SNOWFLAKE_USER=ABDELFATTAH_ABOUELAOUALIM
SNOWFLAKE_PASSWORD=***
SNOWFLAKE_ROLE=DATA_ENGINEER
SNOWFLAKE_WAREHOUSE=TRANSFORM_WH
SNOWFLAKE_DATABASE_DEV=DWH_DEV_ABDELFATTAH
SNOWFLAKE_DATABASE_PROD=DWH_PROD_ABDELFATTAH
SNOWFLAKE_PROD_USER=github_actions_bot
SNOWFLAKE_PROD_PASSWORD=***
```

#### Environnement Protection (PROD)

Dans GitHub Settings â†’ Environments â†’ production:

- âœ… Required reviewers: 1+ approver (Product Owner, Lead Engineer)
- âœ… Wait timer: 0 minutes (immediate aprÃ¨s approval)
- âœ… Deployment branches: `main` only

### Flyway Integration

Flyway gÃ¨re **exclusivement le DDL** (aucun DDL direct via dbt ou SQL manuel).

**ExÃ©cution dans CI/CD:**
```yaml
- name: Run Flyway migrations (PROD)
  uses: docker://flyway/flyway:latest
  with:
    args: >
      -url=jdbc:snowflake://ACCOUNT.snowflakecomputing.com
      -user=${{ secrets.SNOWFLAKE_PROD_USER }}
      -password=${{ secrets.SNOWFLAKE_PROD_PASSWORD }}
      -locations=filesystem:./flyway/sql
      -schemas=staging,intermediate,marts,observability
      migrate
```

**Ordre d'exÃ©cution PROD:**
1. Flyway migrate (DDL: CREATE SCHEMA, GRANT, etc.)
2. dbt run (DML: INSERT, UPDATE via transformations)
3. dbt test (validation)

### Rollback ProcÃ©dure

En cas d'Ã©chec PROD:

1. **Automatic:**
   - dbt test --fail-fast arrÃªte immÃ©diatement le dÃ©ploiement
   - Aucune transaction n'est committÃ©e si un test Ã©choue

2. **Manual:**
   ```bash
   # Revenir Ã  la version prÃ©cÃ©dente
   git revert <commit_sha>
   git push origin main

   # OU rollback Flyway
   flyway repair
   flyway undo  # Si Flyway Teams Edition

   # RedÃ©ployer ancienne version dbt
   dbt run --target prod --full-refresh --vars '{rollback: true}'
   ```

### Changelog et Tags

Chaque dÃ©ploiement PROD crÃ©e un tag Git:

```bash
deploy-20251108-143052
deploy-20251109-091234
```

Permet de tracer exactement quelle version est en prod Ã  tout moment.

---

## Principes de Priorisation et Planification Data

### Framework de Priorisation

Nous utilisons une **matrice pondÃ©rÃ©e multi-critÃ¨res** pour prioriser les dÃ©veloppements data.

#### CritÃ¨res de Scoring (1-5)

| CritÃ¨re | Poids | Description |
|---------|-------|-------------|
| **Impact MÃ©tier** | 35% | Revenus directs, dÃ©cisions stratÃ©giques, OKRs |
| **Urgence RÃ©glementaire** | 25% | RGPD, SOX, compliance, audits |
| **Dette Technique** | 20% | Risque production, maintenabilitÃ©, scalabilitÃ© |
| **Effort DÃ©veloppement** | 15% | Story points, complexitÃ©, dÃ©pendances |
| **Alignement StratÃ©gique** | 5% | Vision long-terme, roadmap produit |

**Score Final = Î£ (CritÃ¨re Ã— Poids)**

**Exemple de Calcul Concret** :

Prenons le cas de **"DÃ©doublonnage TH vs GI avec fuzzy matching"** :

| CritÃ¨re | Note (1-5) | Poids | Calcul | Score PondÃ©rÃ© |
|---------|------------|-------|--------|---------------|
| **Impact MÃ©tier** | 5 | 35% | 5 Ã— 0.35 | **1.75** |
| **Urgence RÃ©glementaire** | 2 | 25% | 2 Ã— 0.25 | **0.50** |
| **Dette Technique** | 4 | 20% | 4 Ã— 0.20 | **0.80** |
| **Effort DÃ©veloppement** | 3 | 15% | 3 Ã— 0.15 | **0.45** |
| **Alignement StratÃ©gique** | 4 | 5% | 4 Ã— 0.05 | **0.20** |
| **TOTAL** | - | **100%** | Î£ | **3.70** |

**DÃ©tail de la Notation** :

1. **Impact MÃ©tier = 5/5** :
   - âœ… 2,684 doublons fusionnÃ©s = -1.82% volumÃ©trie
   - âœ… Golden records = qualitÃ© accrue pour analyses mÃ©tier
   - âœ… Impact direct sur OKR "Data Quality >95%"
   - âš ï¸ Sans dÃ©doublonnage â†’ Reporting KPIs faussÃ©

2. **Urgence RÃ©glementaire = 2/5** :
   - âš ï¸ Pas de deadline lÃ©gale immÃ©diate
   - âœ… Mais qualitÃ© donnÃ©es impacte audits futurs
   - âŒ Pas de conformitÃ© RGPD/SOX bloquante

3. **Dette Technique = 4/5** :
   - âœ… Doublons crÃ©ent risque production (confusion mÃ©tier)
   - âœ… Maintenance difficile (2 sources non rÃ©conciliÃ©es)
   - âš ï¸ Pas de risque crash systÃ¨me immÃ©diat

4. **Effort DÃ©veloppement = 3/5** :
   - âš ï¸ 3 Story Points estimÃ©s (complexitÃ© moyenne)
   - âœ… Macros existantes rÃ©utilisables (text_similarity, haversine)
   - âš ï¸ NÃ©cessite optimisation performance (CROSS JOIN â†’ prÃ©-filtrage INSEE)

5. **Alignement StratÃ©gique = 4/5** :
   - âœ… Roadmap 2025: "RÃ©fÃ©rentiel unique magasins"
   - âœ… Base pour features futures (ML, API temps rÃ©el)

**Score Final = 3.70 â†’ PrioritÃ© P1 (High)**

**DÃ©cision** : Sprint suivant, livraison <1 mois âœ…

---

**Contre-Exemple : "API temps rÃ©el magasins"**

| CritÃ¨re | Note (1-5) | Poids | Calcul | Score PondÃ©rÃ© |
|---------|------------|-------|--------|---------------|
| Impact MÃ©tier | 3 | 35% | 3 Ã— 0.35 | 1.05 |
| Urgence RÃ©glementaire | 1 | 25% | 1 Ã— 0.25 | 0.25 |
| Dette Technique | 1 | 20% | 1 Ã— 0.20 | 0.20 |
| Effort DÃ©veloppement | 2 | 15% | 2 Ã— 0.15 | 0.30 |
| Alignement StratÃ©gique | 3 | 5% | 3 Ã— 0.05 | 0.15 |
| **TOTAL** | - | 100% | Î£ | **1.95** |

**Score Final = 1.95 â†’ PrioritÃ© P3 (Low)**

**DÃ©cision** : Backlog long-terme, pas de besoin mÃ©tier validÃ©, effort non justifiÃ© âŒ

#### Seuils de Priorisation

| Score | PrioritÃ© | Action |
|-------|----------|--------|
| 4.0 - 5.0 | **P0 (Critical)** | Sprint en cours, livraison <2 semaines |
| 3.0 - 3.9 | **P1 (High)** | Prochain sprint, livraison <1 mois |
| 2.0 - 2.9 | **P2 (Medium)** | Backlog proche, livraison <3 mois |
| 1.0 - 1.9 | **P3 (Low)** | Backlog long-terme, livraison >3 mois |
| <1.0 | **P4 (Nice-to-have)** | Candidate pour dÃ©prioritisation |

### RÃ¨gles d'Arbitrage

#### 1. Nouvelle Source vs Dette Technique

```
SI dette_technique.risque_prod == HIGH:
    prioritÃ© = P0 (dette technique)
SINON SI nouvelle_source.revenue_direct > seuil:
    prioritÃ© = P1 (nouvelle source)
SINON:
    score = balance(impact_mÃ©tier, risque_technique)
```

#### 2. Maintenance vs Feature

```
maintenance_critique (P0) > feature (P1)
maintenance_mineure (P2) < feature_haute_valeur (P1)

Budget mensuel:
  - 70% features nouvelles
  - 30% maintenance + dette technique
```

#### 3. Compliance RÃ©glementaire

```
SI compliance IN [RGPD, SOX, HIPAA]:
    prioritÃ© = P0 (override automatique)
SI compliance == nice_to_have:
    prioritÃ© = P2
```

### Roadmap Actuelle (Exemple)

#### MUST HAVE

| Item | PrioritÃ© | Score | Effort | Livraison |
|------|----------|-------|--------|-----------|
| DÃ©doublonnage inter-sources (TH vs GI) | P0 | 4.2 | 3 SP | 2025-01-15 |
| CI/CD GitHub Actions complet | P0 | 4.5 | 2 SP | âœ… Done |
| RBAC Snowflake production | P1 | 4.1 | 2 SP | âœ… Done |
| Tests dbt avancÃ©s | P1 | 3.8 | 1 SP | âœ… Done |

#### SHOULD HAVE

| Item | PrioritÃ© | Score | Effort | Livraison |
|------|----------|-------|--------|-----------|
| Nouvelle source NX (45k magasins) | P1 | 3.9 | 5 SP | 2025-04-30 |
| Optimisation perf CROSS JOIN (15minâ†’2min) | P2 | 3.2 | 3 SP | 2025-05-15 |
| Data quality monitoring (Great Expectations) | P2 | 3.1 | 2 SP | 2025-06-30 |

#### Backlog Long-Terme

| Item | PrioritÃ© | Score | Raison |
|------|----------|-------|--------|
| Machine Learning matching (vs fuzzy) | P3 | 2.8 | AmÃ©lioration incrÃ©mentale, ROI incertain |
| API temps rÃ©el | P3 | 2.5 | Pas de besoin mÃ©tier validÃ© |
| Multi-pays (hors France) | P4 | 1.9 | Hors scope actuel |

### Processus de Planification

#### Sprint Planning (Bi-hebdomadaire)

1. **Lundi Semaine 1:** Review backlog avec Product Owner
2. **Mardi Semaine 1:** Scoring nouvelles demandes
3. **Mercredi Semaine 1:** SÃ©lection items sprint (capacity 20 SP/ingÃ©nieur)
4. **Lundi Semaine 2:** Daily meeting
5. **Vendredi Semaine 2:** Sprint review + dÃ©mo

**Explication "capacity 20 SP/ingÃ©nieur"** :

La **capacity** (capacitÃ©) reprÃ©sente le **nombre de Story Points (SP) qu'un ingÃ©nieur peut rÃ©aliser pendant un sprint** de 2 semaines.

**Story Points (SP)** :
- UnitÃ© abstraite mesurant la **complexitÃ©** d'une tÃ¢che (pas le temps)
- Ã‰chelle Fibonacci: 1, 2, 3, 5, 8, 13, 21
- Calibrage Ã©quipe:
  - **1 SP** = TÃ¢che trÃ¨s simple (ex: Ajouter un commentaire dans une macro)
  - **2 SP** = TÃ¢che simple (ex: Ajouter un test dbt de base)
  - **3 SP** = TÃ¢che moyenne (ex: CrÃ©er nouveau modÃ¨le staging)
  - **5 SP** = TÃ¢che complexe (ex: ImplÃ©menter fuzzy deduplication avec optimisation)
  - **8 SP** = TÃ¢che trÃ¨s complexe (ex: Nouvelle source avec transformations complÃ¨tes)
  - **13+ SP** = Ã€ dÃ©couper (trop complexe pour un sprint)

**Capacity 20 SP/ingÃ©nieur sur 2 semaines** signifie:
- Sprint bi-hebdomadaire (10 jours ouvrÃ©s)
- **Jours disponibles = 10 jours**
- **DÃ©ductions** :
  - RÃ©unions (daily, planning, review, retro) : ~1.5 jours
  - Support/bugs ad-hoc : ~1 jour
  - Temps libre/crÃ©ativitÃ© : ~0.5 jour
- **Jours dÃ©veloppement effectif = 7 jours**
- **Capacity thÃ©orique = 20 SP** (~3 SP/jour effectif)

**Exemple Sprint RÃ©aliste** :

| Task | SP | AssignÃ© | Justification |
|------|----|---------|--------------|
| GPS validation via INSEE | 5 | Alice | Macro regex + matching fuzzy + tests |
| Fuzzy deduplication TH vs GI | 8 | Alice | Optimisation CROSS JOIN complexe |
| Nouvelle source NX (staging) | 3 | Bob | Typage + nettoyage classique |
| Tests dbt avancÃ©s (expectations) | 2 | Bob | Ajout dbt_expectations |
| Bug: Fix SCD2 duplicate is_current | 2 | Alice | Investigation + fix + test |
| **TOTAL ALICE** | **15 SP** | - | 75% capacity (marge sÃ©curitÃ©) |
| **TOTAL BOB** | **5 SP** | - | 25% capacity (onboarding, Bob nouveau) |

**Raisons Capacity <100%** :
- âš ï¸ **ImprÃ©vus** : Bugs production, support urgent mÃ©tier
- âš ï¸ **Estimation incertaine** : TÃ¢ches parfois plus complexes que prÃ©vu
- âš ï¸ **Contexte switch** : Interruptions, meetings non planifiÃ©es
- âš ï¸ **Onboarding** : Nouveaux arrivants ont capacity rÃ©duite (5-10 SP/sprint)

**Ajustement Capacity** :
- **Sprint 1-2** : Mesurer vÃ©locitÃ© rÃ©elle (SP complÃ©tÃ©s vs SP planifiÃ©s)
- **Sprint 3+** : Ajuster capacity (ex: Si Alice complÃ¨te systÃ©matiquement 18 SP â†’ capacity = 18)
- **Revue trimestrielle** : Re-calibrer complexitÃ© SP (Ã©volution compÃ©tences Ã©quipe)

#### Demande Urgente (Hotfix)

Si demande mÃ©tier urgente:
1. Ã‰valuation impact (blocker business?)
2. Si score >4.0 â†’ fast-track P0
3. Sinon â†’ prochaine sprint review

#### RÃ©vision Roadmap (Trimestrielle)

- Mise Ã  jour scores en fonction Ã©volution mÃ©tier
- RÃ©Ã©valuation dette technique accumulÃ©e
- Ajustement capacity team

### MÃ©triques de SuccÃ¨s

| MÃ©trique | Cible | Actuel |
|----------|-------|--------|
| VÃ©locitÃ© sprint (SP/semaine) | 20 | - |
| Lead time (P0) | <2 semaines | - |
| Taux de rÃ©ussite dÃ©ploiement PROD | >95% | 100% (early) |
| Dette technique ratio | <20% backlog | 15% |

---

## Guide de DÃ©marrage Rapide

### PrÃ©requis

- Python 3.10+
- Git
- AccÃ¨s Snowflake (compte + rÃ´le DATA_ENGINEER ou DBT_RUNNER)
- Flyway CLI (optionnel, sinon via Docker)

### Installation Locale

#### PrÃ©requis

- Python 3.10 ou supÃ©rieur
- Git
- AccÃ¨s Snowflake avec rÃ´le DATA_ENGINEER ou DBT_RUNNER

#### 1. Clone du repo

```bash
git clone https://github.com/your-org/gg-vp-data-exercice.git
cd gg-vp-data-exercice
```

#### 2. Installation de UV (Package Manager Rapide)

**UV** est un package installer Python ultra-rapide (10-100x plus rapide que pip). Il remplace avantageusement `venv + pip`.

**MacOS** :
```bash
# Via Homebrew (recommandÃ©)
brew install uv

# Ou via curl
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Linux** :
```bash
# Via curl (compatible toutes distributions)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Ou via pip (si Python dÃ©jÃ  installÃ©)
pip install uv
```

**Windows** :
```powershell
# Via PowerShell (recommandÃ©)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Ou via pip (si Python dÃ©jÃ  installÃ©)
pip install uv

# Ou via Scoop
scoop install uv
```

VÃ©rifier installation :
```bash
uv --version
# Attendu: uv 0.1.x ou supÃ©rieur
```

#### 3. Setup Environnement Python avec UV

**Toutes plateformes (MacOS, Linux, Windows)** :

```bash
# CrÃ©er environnement virtuel (.venv) + installer dÃ©pendances en une commande
uv venv
uv pip install dbt-snowflake==1.10.3

# Activer l'environnement virtuel
# MacOS/Linux:
source .venv/bin/activate

# Windows (CMD):
.venv\Scripts\activate.bat

# Windows (PowerShell):
.venv\Scripts\Activate.ps1
```

**Avantages UV vs pip classique** :
- âš¡ **10-100x plus rapide** (parallÃ©lisation native)
- ğŸ”’ **Lockfile automatique** (reproductibilitÃ© garantie)
- ğŸ“¦ **Cache global** (partage packages entre projets)
- ğŸ¯ **RÃ©solution conflits intelligente**

**Alternative : requirements.txt (recommandÃ© pour CI/CD)** :

```bash
# CrÃ©er requirements.txt
cat > requirements.txt <<EOF
dbt-snowflake==1.10.3
sqlfluff==2.3.0
EOF

# Installer via UV (ultra-rapide)
uv venv
uv pip install -r requirements.txt

# Activer environnement
source .venv/bin/activate  # MacOS/Linux
.venv\Scripts\activate     # Windows
```

#### 4. Configuration profiles.yml
   ```bash
   mkdir -p ~/.dbt
   cat > ~/.dbt/profiles.yml <<EOF
   gg_vp_data:
     target: dev
     
     outputs:
     dev:
       type: snowflake
       account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
       user: "{{ env_var('SNOWFLAKE_USER') }}"
       password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
       role: "{{ env_var('SNOWFLAKE_ROLE') }}"
       database: "{{ env_var('SNOWFLAKE_DATABASE_DEV') }}"
       warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
       schema: staging
       threads: 4
       client_session_keep_alive: False
   EOF
   ```

4. **Install dbt packages:**
   ```bash
   dbt deps
   ```

5. **Test connexion:**
   ```bash
   dbt debug
   ```

### Premier Run

```bash
# Seed rÃ©fÃ©rentiel communes INSEE France 2025
dbt seed

# Run tous les modÃ¨les
dbt run

# Run tests
dbt test

# GÃ©nÃ©rer documentation
dbt docs generate
dbt docs serve  # Ouvre http://localhost:8080
```

**Visualisation de la Documentation dbt** :

Une fois `dbt docs serve` lancÃ©, ouvrez http://localhost:8080 dans votre navigateur.

**1. Graphe de Lineage (DÃ©pendances des ModÃ¨les)** :

![Lineage Graph](./images/lineage.png)

Le **lineage graph** montre les dÃ©pendances entre tous les modÃ¨les :
- **Sources** (TH.magasins, GI.magasins) â†’ **Staging** â†’ **Intermediate** â†’ **Marts**
- Visualisation des transformations successives
- Identification rapide de l'impact d'un changement (upstream/downstream)

**Navigation** :
- Cliquez sur un nÅ“ud pour voir ses dÃ©tails
- Utilisez le bouton "Focus" pour centrer sur un modÃ¨le spÃ©cifique
- Filtrez par tag, schÃ©ma, ou type de ressource

**2. DÃ©tails d'un ModÃ¨le (dim_magasin)** :

![Model Details](./images/details.png)

La page de dÃ©tails affiche :
- **Description** complÃ¨te du modÃ¨le (depuis `schema.yml`)
- **Colonnes** avec types et descriptions
- **Tests** configurÃ©s (not_null, unique, relationships, etc.)
- **Code SQL** source compilÃ©
- **Statistiques** de build (lignes, temps d'exÃ©cution)
- **Lineage** upstream/downstream

**Exemple Utilisation** :
```bash
# 1. GÃ©nÃ©rer docs aprÃ¨s modification
dbt docs generate

# 2. Servir localement
dbt docs serve --port 8080

# 3. Ou gÃ©nÃ©rer site statique pour dÃ©ploiement
dbt docs generate
# â†’ GÃ©nÃ¨re target/index.html + target/catalog.json

# 4. HÃ©berger sur GitHub Pages, S3, etc.
cp -r target/ docs-site/
# â†’ DÃ©ployer docs-site/ sur hÃ©bergement statique
```

**Captures d'Ã‰cran dbt Docs** :

Les images `./images/lineage.png` et `./images/details.png` sont gÃ©nÃ©rÃ©es en :
1. LanÃ§ant `dbt docs serve`
2. Ouvrant http://localhost:8080
3. Naviguant vers le lineage graph (bouton "View DAG" en haut Ã  droite)
4. Prenant une capture d'Ã©cran (lineage.png)
5. Cliquant sur le modÃ¨le `dim_magasin`
6. Prenant une capture d'Ã©cran de la page de dÃ©tails (details.png)

---

## DÃ©veloppement Feature

1. **CrÃ©er branche:**
   ```bash
   git checkout develop
   git pull origin develop
   git checkout -b feature/nouvelle-source-nx
   ```

2. **DÃ©velopper + tester localement:**
   ```bash
   dbt run --select +nouvelle_source_nx
   dbt test --select +nouvelle_source_nx
   ```

3. **Lint SQL:**
   ```bash
   sqlfluff lint models/ --dialect snowflake
   ```

4. **Commit + Push (Conventional Commits):**

Le projet suit la spÃ©cification **Conventional Commits** pour standardiser les messages de commit et gÃ©nÃ©rer automatiquement le CHANGELOG.

**Format** : `<type>(<scope>): <description courte>`

**Types Standard** :

| Type | Usage | Exemples |
|------|-------|----------|
| `feat` | Nouvelle fonctionnalitÃ© | Nouveau modÃ¨le, nouvelle source, nouvelle macro |
| `fix` | Correction bug | Fix SCD2, fix test Ã©chouÃ©, fix erreur SQL |
| `refactor` | Refactoring (pas de changement fonctionnel) | Optimisation query, nettoyage code |
| `docs` | Documentation uniquement | Mise Ã  jour README, ajout ADR |
| `test` | Ajout/modification tests | Nouveaux tests dbt_expectations |
| `perf` | AmÃ©lioration performance | Optimisation CROSS JOIN, ajout clustering |
| `chore` | TÃ¢ches diverses (deps, config) | Mise Ã  jour dbt packages, config CI/CD |
| `ci` | Modification CI/CD | Ajout GitHub Actions step |

**Exemples Concrets** :

```bash
# âœ… Nouvelle fonctionnalitÃ©
git commit -m "feat(dedup): add fuzzy matching deduplication with INSEE filtering"

# âœ… Nouvelle fonctionnalitÃ© avec body dÃ©taillÃ©
git commit -m "feat(gps): add GPS validation via INSEE communes referential

- Extract city from store name with regex patterns
- Fuzzy match against INSEE communes database
- Auto-correct GPS if >10km error detected
- Add anomaly levels: CRITIQUE, MAJEURE, MINEURE, OK

Closes #42"

# âœ… Correction de bug
git commit -m "fix(dedup): use latitude_corrigee instead of latitude in fuzzy matching

Previous implementation used original GPS coordinates which contained errors.
Now using corrected coordinates from int_magasins_geo_validated model.

Fixes #53"

# âœ… Refactoring (pas de changement fonctionnel)
git commit -m "refactor(staging): simplify stg_th_magasins column typing logic"

# âœ… Documentation
git commit -m "docs: add ADR-004 for GPS validation strategy

Explains decision to use INSEE communes referential for GPS correction
instead of Google Maps API or manual correction."

# âœ… Tests
git commit -m "test(marts): add dbt_expectations tests for dim_magasin SCD2 integrity"

# âœ… Performance
git commit -m "perf(dedup): optimize fuzzy matching with code_insee pre-filtering

Reduces CROSS JOIN from 6.1B to 1.86M comparisons (99.97% reduction).
Build time decreased from 45min to 6min."

# âœ… CI/CD
git commit -m "ci: add manual approval for PROD deployments

Adds GitHub environment protection with required reviewers before
deploying to production database."

# âœ… Breaking change (MAJEUR)
git commit -m "feat(dedup)!: change dim_magasin schema to include merge metadata

BREAKING CHANGE: Added columns sources_merged, merge_name_similarity,
merge_distance_km. Downstream dashboards need to be updated."

# âŒ Mauvais exemples (Ã  Ã©viter)
git commit -m "update stuff"  # âŒ Trop vague
git commit -m "wip"           # âŒ Work in progress (ne pas committer)
git commit -m "fix bug"       # âŒ Quel bug?
git commit -m "asdf"          # âŒ Message inutile
```

**Workflow Git Complet** :

```bash
# 1. CrÃ©er branche depuis develop
git checkout develop
git pull origin develop
git checkout -b feat/gps-validation-insee

# 2. Faire les modifications
# ... dÃ©veloppement ...

# 3. Tester localement
dbt run --select +int_magasins_geo_validated
dbt test --select +int_magasins_geo_validated

# 4. Commit avec Conventional Commits
git add models/intermediate/int_magasins_geo_validated.sql
git add macros/extract_city_from_name.sql
git add tests/assert_gps_anomaly_levels.sql

git commit -m "feat(gps): add GPS validation via INSEE communes referential

- Extract city from store name with 3 regex patterns
- Fuzzy match against INSEE communes (34k records)
- Calculate Haversine distance to commune center
- Auto-correct if >10km anomaly detected
- Add anomaly classification (CRITIQUE/MAJEURE/MINEURE/OK)

Results:
- 80.9% stores have GPS anomalies
- 30.4% auto-corrected
- 19.1% validated OK

Co-authored-by: Product Team <product@company.com>
Closes #42"

# 5. Push vers remote
git push origin feat/gps-validation-insee

# 6. CrÃ©er Pull Request sur GitHub
# â†’ CI/CD va automatiquement lint + test
```

**Outils RecommandÃ©s** :

```bash
# Installer commitlint (validation automatique)
npm install --save-dev @commitlint/{cli,config-conventional}

# Installer husky (Git hooks)
npm install --save-dev husky
npx husky add .husky/commit-msg 'npx --no -- commitlint --edit "$1"'

# DÃ©sormais, commits non-conformes seront rejetÃ©s
git commit -m "bad commit"
# âŒ Error: Commit message does not follow Conventional Commits
```

5. **CrÃ©er Pull Request:**
   - Target: `develop`
   - CI/CD va automatiquement: lint â†’ test â†’ deploy DEV

6. **Merge to Production:**
   - Merge `develop` â†’ `main`
   - Attendre approval manuelle
   - CI/CD dÃ©ploie en PROD

---

## Maintenance et Exploitation

### Monitoring Production

#### Dashboards RecommandÃ©s

1. **Snowflake Web UI:**
   - Query History: Identifier requÃªtes lentes
   - Warehouse Usage: Optimiser coÃ»ts compute
   - Data Storage: Monitorer croissance volumÃ©trie

2. **dbt Docs (auto-gÃ©nÃ©rÃ©):**
   - Lineage graph: Comprendre dÃ©pendances modÃ¨les
   - Column descriptions: Documentation mÃ©tier
   - Test results: Ã‰tat qualitÃ© donnÃ©es

3. **GitHub Actions:**
   - Workflow runs: Historique dÃ©ploiements
   - Test results: Trends qualitÃ©

#### Alertes Critiques

| Alerte | Seuil | Action |
|--------|-------|--------|
| dbt run failed (PROD) | 1 failure | Page on-call engineer |
| Test failure rate >10% | 10% | Investigate data quality |
| Freshness >24h | 24h | Check source systems |
| Warehouse credit usage >budget | 90% budget | Optimize queries |

### Runbook: Incidents Courants

#### 1. "dbt run failed: connection timeout"

**Cause:** Snowflake warehouse suspendu ou surchargÃ©

**Solution:**
```bash
# VÃ©rifier statut warehouse
SHOW WAREHOUSES LIKE 'TRANSFORM_WH';

# RedÃ©marrer si nÃ©cessaire
ALTER WAREHOUSE TRANSFORM_WH RESUME;

# Retry dbt run
dbt run --target prod
```

#### 2. "Test failed: assert_scd2_one_current_per_store"

**Cause:** Doublon is_current=TRUE (intÃ©gritÃ© SCD2 violÃ©e)

**Solution:**
```sql
-- Identifier les doublons
SELECT magasin_id, source_system, COUNT(*)
FROM DWH_PROD_ABDELFATTAH.marts.dim_magasin
WHERE is_current = TRUE
GROUP BY 1,2
HAVING COUNT(*) > 1;

-- Corriger manuellement (garder le plus rÃ©cent)
UPDATE DWH_PROD_ABDELFATTAH.marts.dim_magasin
SET is_current = FALSE
WHERE magasin_key = '<old_key>';

-- Re-run dbt test
dbt test --select dim_magasin
```

#### 3. "Flyway migration failed: schema already exists"

**Cause:** Migration dÃ©jÃ  appliquÃ©e ou Ã©tat inconsistant

**Solution:**
```bash
# Check migration history
flyway info

# Repair metadata (si besoin)
flyway repair

# Retry migration
flyway migrate
```

### Backup et Disaster Recovery

#### Backups Snowflake (Automatique)

- **Time Travel**: 1 jour (gratuit)
- **Fail-Safe**: 7 jours additionnels (Snowflake gÃ¨re)

**Restore example:**
```sql
-- Restore table Ã  T-2h
CREATE TABLE dim_magasin_restored CLONE dim_magasin
  AT(OFFSET => -7200);  -- 2 heures = 7200 secondes

-- VÃ©rifier donnÃ©es
SELECT COUNT(*) FROM dim_magasin_restored;

-- Remplacer si OK
DROP TABLE dim_magasin;
ALTER TABLE dim_magasin_restored RENAME TO dim_magasin;
```

#### Backup Code (Git)

- Tags de dÃ©ploiement PROD = backup code
- Branches archivÃ©es conservÃ©es indÃ©finiment

### Performance Optimization

#### Query Optimization

1. **Analyser query profile:**
   ```sql
   -- Dans Snowflake UI: Query History â†’ Query Profile
   ```

2. **MatÃ©rialiser vues frÃ©quemment requÃªtÃ©es:**
   ```yaml
   # dbt_project.yml
   models:
     gg_vp_data:
       intermediate:
         +materialized: table  # Au lieu de view
   ```

3. **Ajouter clustering keys (tables >1GB):**
   ```sql
   ALTER TABLE dim_magasin CLUSTER BY (source_system, is_current);
   ```

#### dbt Optimization

```bash
# Run incrÃ©mental uniquement (pas full-refresh)
dbt run --select dim_magasin

# ParallÃ©liser (utiliser tous les threads)
dbt run --threads 8

# Run subset de modÃ¨les
dbt run --select +dim_magasin  # dim_magasin + upstreams
dbt run --select dim_magasin+  # dim_magasin + downstreams
```

---

## RÃ©fÃ©rences et Ressources

### Documentation Externe

- [dbt Documentation](https://docs.getdbt.com/)
- [Snowflake Documentation](https://docs.snowflake.com/)
- [Flyway Documentation](https://flywaydb.org/documentation/)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

### Documentation Interne

- [ADR-001: Choix SCD Type 2](docs/ADR-001-scd-type-2.md)
- [ADR-002: StratÃ©gie Matching Fuzzy](docs/ADR-002-matching-strategy.md)
- [ADR-003: DÃ©doublonnage Inter-Sources](docs/ADR-003-deduplication-fuzzy-matching.md)
- [ADR-004: â­ Validation GPS via INSEE](docs/ADR-004-gps-validation-correction.md) â† **Nouveau!**
- [Roadmap Data](docs/ROADMAP.md)

### Support

- **Ã‰quipe Data Engineering:** #data-engineering (Slack)
- **Incidents Production:** PagerDuty â†’ on-call engineer
- **Demandes MÃ©tier:** Trello board "Data Requests"

---

**Version:** 1.0.0
**DerniÃ¨re RÃ©vision:** 2025-11-08
**Maintenu par:** Data Engineering Team
